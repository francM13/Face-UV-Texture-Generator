{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "batch_size = 32\n",
    "input_path = \"D:\\Tesi Dataset\\Z_Test_Dataset\\\\256_resized_Face\"\n",
    "output_path= \"D:\\Tesi Dataset\\Z_Test_Dataset\\\\256_resized_UV\"\n",
    "\n",
    "training_size = 0.7\n",
    "random_seed=13\n",
    "steps_per_epoch=None\n",
    "steps_per_validation=None\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_path, fname)\n",
    "        for fname in os.listdir(input_path)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(output_path, fname)\n",
    "        for fname in os.listdir(output_path)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "if len(input_img_paths) != len(target_img_paths):\n",
    "    raise ValueError(\"The number of images doesn't match.\")\n",
    "print(\"Number of samples:\", len(input_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "from tensorflow import io as tf_io\n",
    "from tensorflow import image as tf_image\n",
    "\n",
    "def get_dataset(\n",
    "    batch_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    \"\"\"Returns a TF Dataset.\"\"\"\n",
    "    def load_img(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=3)\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"float32\")\n",
    "\n",
    "        return input_img, target_img\n",
    "\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "random.Random(random_seed).shuffle(input_img_paths)\n",
    "random.Random(random_seed).shuffle(target_img_paths)\n",
    "\n",
    "K=int(len(input_img_paths)*training_size/batch_size)*batch_size\n",
    "T=int(len(input_img_paths)*(training_size+(1-training_size)/2)/batch_size)*batch_size\n",
    "\n",
    "train_input_img_paths = input_img_paths[:K]\n",
    "train_target_img_paths = target_img_paths[:K]\n",
    "\n",
    "val_input_img_paths = input_img_paths[K:T]\n",
    "val_target_img_paths = target_img_paths[K:T]\n",
    "\n",
    "test_input_img_paths = input_img_paths[T:]\n",
    "test_target_img_paths = target_img_paths[T:]\n",
    "\n",
    "train_dataset = get_dataset(\n",
    "    batch_size,\n",
    "    train_input_img_paths,\n",
    "    train_target_img_paths,\n",
    ")\n",
    "valid_dataset = get_dataset(\n",
    "    batch_size, val_input_img_paths, val_target_img_paths\n",
    ")\n",
    "test_dataset = get_dataset(\n",
    "    1, test_input_img_paths, test_target_img_paths\n",
    ")\n",
    "\n",
    "print(\"Size Train Set: \",len(train_dataset),\"\\nSize Valid Set: \",len(valid_dataset),\"\\nSize Test Set: \",len(test_dataset))\n",
    "\n",
    "# train_dataset=train_dataset.repeat()\n",
    "# valid_dataset=valid_dataset.repeat()\n",
    "# test_dataset=test_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  Conv2D, Conv2DTranspose, SpatialDropout2D\n",
    "model = keras.Sequential()\n",
    "\n",
    "def encoder_block(model, num_filters, depth=3):\n",
    "    model.add(Conv2D(num_filters, (3, 3),strides=2, activation='relu', padding='same',name=f\"{num_filters}_Conv_1\"))\n",
    "    \n",
    "    for i in range(depth-1):\n",
    "        model.add(Conv2D(num_filters, (3, 3), activation='relu', padding='same',name=f\"{num_filters}_Conv_{i+2}\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def decoder_block(model, num_filters, depth=3):\n",
    "    model.add(Conv2DTranspose(num_filters, (3, 3),strides=2, activation='relu', padding='same',name=f\"{num_filters}_T-Conv_1\"))\n",
    "\n",
    "    for i in range(depth-1):\n",
    "        model.add(Conv2DTranspose(num_filters, (3, 3), activation='relu', padding='same',name=f\"{num_filters}_T-Conv_{i+2}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def autoencoder():\n",
    "    # Encoder\n",
    "    model.add(Conv2D(3, (3, 3), activation='relu', padding='same', input_shape=(256, 256,3),name=\"input\"))\n",
    "    \n",
    "    encoder_block(model, 32,depth=3)\n",
    "    encoder_block(model, 64,depth=5)\n",
    "    encoder_block(model, 128,depth=7)\n",
    "    encoder_block(model, 256,depth=9)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_block(model, 256,depth=9)\n",
    "    decoder_block(model, 128,depth=7)\n",
    "    decoder_block(model, 64,depth=5)\n",
    "    decoder_block(model, 32,depth=3)\n",
    "\n",
    "    model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same',name=\"output\"))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adamax', loss='binary_crossentropy',\n",
    "                  metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = autoencoder()\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, to_file=\"autoencoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=15),\n",
    "    keras.callbacks.ModelCheckpoint(\"checkpoint/Interim_Model.keras\", save_best_only=True),\n",
    "    keras.callbacks.CSVLogger(\"checkpoint/Logger\"),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=steps_per_validation,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.image_plot import image_plot\n",
    "from itertools import islice\n",
    "import numpy as np  \n",
    "\n",
    "for j,k in islice(zip(test_dataset, predictions), 10):\n",
    "    image_plot(np.concatenate((j[1].numpy()[0], k), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texture-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
